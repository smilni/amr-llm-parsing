{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a7585ef",
      "metadata": {},
      "source": [
        "## Preparation: imports, global vars, mounting drive (if on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "36b6e879",
      "metadata": {
        "id": "36b6e879"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import amrlib\n",
        "import re\n",
        "from typing import Tuple, List, Sequence, Dict\n",
        "from tqdm.notebook import tqdm\n",
        "import openai\n",
        "from datetime import datetime\n",
        "from io import StringIO\n",
        "import penman\n",
        "from penman import Graph\n",
        "from penman.exceptions import PenmanError\n",
        "from smatchpp import Smatchpp, solvers, preprocess, eval_statistics\n",
        "from smatchpp.formalism.amr import tools as amrtools\n",
        "import os, json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4E1sV0Oon7BT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E1sV0Oon7BT",
        "outputId": "b378375c-e6ed-4053-fa55-117dbe9b3095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c7cfef",
      "metadata": {
        "id": "d5c7cfef"
      },
      "source": [
        "## Load test dataset (AMR 3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5539dc",
      "metadata": {
        "id": "ea5539dc"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_parquet(\"./data/test-00000-of-00001.parquet\")\n",
        "df_train = pd.read_parquet(\"./data/train-00000-of-00001.parquet\")\n",
        "\n",
        "sample_test = df_test.conversations.sample(50, random_state=42)\n",
        "sample_train = df_train.conversations.sample(10, random_state=42)\n",
        "full_train = df_train.conversations\n",
        "full_train_texts = [item[0].get(\"content\", \"\").replace(\"Generate an Abstract Meaning Representation (AMR) graph for the following sentence: \", \"\") for item in full_train]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76fd1dab",
      "metadata": {
        "id": "76fd1dab"
      },
      "source": [
        "## Load AMR parsing model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65327eb9",
      "metadata": {
        "id": "65327eb9"
      },
      "source": [
        "After a long and exhausting battle with Homebrew and various versions, I found a specific distribution of mip that must be installed for smatchpp to function properly. Without this, as far as I understand, you are doomed to get cbclib-related errors, at least on Mac with Apple Silicon (M1 in my case). This is an important step without which running parsing models locally won't work.\n",
        "\n",
        "Source: https://github.com/coin-or/python-mip/issues/335"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707e48fc",
      "metadata": {
        "id": "707e48fc"
      },
      "outputs": [],
      "source": [
        "! pip3 install mip==1.16rc0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777a9567",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fe89857bfd1f447088531fb0410c7f00"
          ]
        },
        "id": "777a9567",
        "outputId": "8d7ef257-cbd2-4d93-a613-e9283ce1bd4b"
      },
      "outputs": [],
      "source": [
        "stog = amrlib.load_stog_model()\n",
        "\n",
        "graphs = stog.parse_sents(['This is a test of the system.', 'This is a second sentence.'])\n",
        "for graph in tqdm(graphs):\n",
        "    print(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bd3347",
      "metadata": {
        "id": "00bd3347"
      },
      "source": [
        "## Define functions for AMR processing and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a48808d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a48808d6",
        "outputId": "0e1778cd-9fed-4e2d-ce8e-be78ed41a469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PENMAN parse error\n",
            "invalid AMR: (x / invalid | reason: ['PENMAN parse error: \\n  line 1\\n    (x / invalid\\n                ^\\nDecodeError: Unexpected end of input']\n",
            "\n",
            "Cbc3007W No integer variables\n",
            "Cbc3007W No integer variables\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'main': {'F1': {'result': np.float64(87.5),\n",
              "   'ci': (np.float64(75.0), np.float64(100.0))},\n",
              "  'Precision': {'result': np.float64(87.5),\n",
              "   'ci': (np.float64(75.0), np.float64(100.0))},\n",
              "  'Recall': {'result': np.float64(87.5),\n",
              "   'ci': (np.float64(75.0), np.float64(100.0))}}}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_standardizer = amrtools.AMRStandardizer()\n",
        "printer = eval_statistics.ResultPrinter(score_type=\"micro\", do_bootstrap=True, output_format=\"json\")\n",
        "ilp = solvers.ILP()\n",
        "measure = Smatchpp(alignmentsolver=ilp, graph_standardizer=graph_standardizer, printer=printer)\n",
        "\n",
        "def validate_amr(amr_str):\n",
        "    \"\"\"\n",
        "    Parse AMR with penman and return a list of validation errors.\n",
        "    An empty list means the AMR passed all checks.\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "\n",
        "    try:\n",
        "        g = penman.decode(amr_str)\n",
        "    except Exception as e:\n",
        "        return [f'PENMAN parse error: {e}']\n",
        "\n",
        "    return errors\n",
        "\n",
        "\n",
        "# AMR variable convention: usually a short lowercase letter with optional digits (e.g., d, x3, person2).\n",
        "AMR_VAR_RE = re.compile(r'^[a-z][a-z0-9]*$')\n",
        "\n",
        "def validate_amr(amr_str):\n",
        "    \"\"\"\n",
        "    Parse AMR with penman and return a list of validation errors.\n",
        "    An empty list means the AMR passed all checks.\n",
        "    \"\"\"\n",
        "    errors: List[str] = []\n",
        "\n",
        "    # Parseability\n",
        "    try:\n",
        "        g = penman.decode(amr_str)\n",
        "    except Exception as e:\n",
        "        print(\"PENMAN parse error\")\n",
        "        return [f'PENMAN parse error: {e}']\n",
        "\n",
        "    triples = g.triples\n",
        "\n",
        "    # Root check\n",
        "    if g.top is None:\n",
        "        errors.append(\"AMR has no top/root variable (missing instance for a root).\")\n",
        "\n",
        "    # Build instance map and variable set\n",
        "    instance_of = {}\n",
        "    var_set = set()\n",
        "\n",
        "    for s, r, t in triples:\n",
        "        var_set.add(s)\n",
        "        if r == ':instance':\n",
        "            if s in instance_of and t:\n",
        "                errors.append(f\"Multiple :instance triples for variable '{s}' \"\n",
        "                              f\"({instance_of[s]} and {t}).\")\n",
        "            instance_of[s] = t\n",
        "\n",
        "    # Every variable must (a) look like a proper AMR variable and (b) have exactly one :instance\n",
        "    for v in var_set:\n",
        "        # (a) variable naming\n",
        "        if not AMR_VAR_RE.fullmatch(v):\n",
        "            errors.append(\n",
        "                f\"Illegal variable name '{v}'. \"\n",
        "                \"Variables must be simple lowercase letters with optional digits (e.g., 'd', 'x3'). \"\n",
        "            )\n",
        "        # (b) instance triple present\n",
        "        if v not in instance_of:\n",
        "            errors.append(\n",
        "                f\"Variable '{v}' has no ':instance' triple (missing 'v / TYPE').\"\n",
        "            )\n",
        "\n",
        "    # Ensure the top variable itself has an :instance\n",
        "    if g.top is not None and g.top not in instance_of:\n",
        "        errors.append(f\"Top variable '{g.top}' has no ':instance' triple.\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "def is_valid_amr(amr_str):\n",
        "    \"\"\"\n",
        "    Checks if AMR is valid and returns (valid, errors).\n",
        "    \"\"\"\n",
        "    errors = validate_amr(amr_str)\n",
        "    return (len(errors) == 0, errors)\n",
        "\n",
        "\n",
        "def validate_and_score(\n",
        "    gold_amrs,\n",
        "    sys_amrs,\n",
        "    report_path = \"amr_validation_report.txt\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Writes a report to `report_path`.\n",
        "    \"\"\"\n",
        "    assert len(gold_amrs) == len(sys_amrs), \"Mismatch in AMR count\"\n",
        "\n",
        "    valid_gold = []\n",
        "    valid_sys = []\n",
        "    valid_count = 0\n",
        "\n",
        "    buf = StringIO()\n",
        "\n",
        "    for gold, sys in zip(gold_amrs, sys_amrs):\n",
        "        ok, *rest = is_valid_amr(sys)\n",
        "        if ok:\n",
        "            valid_count += 1\n",
        "            valid_gold.append(gold)\n",
        "            valid_sys.append(sys)\n",
        "        else:\n",
        "            reason = f\" | reason: {rest[0]}\" if rest else \"\"\n",
        "            buf.write(f\"invalid AMR: {sys}{reason}\\n\")\n",
        "            print(f\"invalid AMR: {sys}{reason}\\n\")\n",
        "\n",
        "    total = len(sys_amrs)\n",
        "    valid_percent = 100.0 * valid_count / total if total else 0.0\n",
        "\n",
        "    # Score only valid !!!\n",
        "    smatch_score, optimization_status = measure.score_corpus(valid_gold, valid_sys)\n",
        "\n",
        "    # Write summary + metrics\n",
        "    buf.write(f\"\\nValid AMRs: {valid_percent:.2f}% ({valid_count}/{total})\\n\")\n",
        "    buf.write(\"\\nMetrics (valid only)\\n\")\n",
        "\n",
        "    main = smatch_score.get(\"main\", {})\n",
        "    for name, d in main.items():\n",
        "        val = float(d[\"result\"])\n",
        "        lo, hi = map(float, d.get(\"ci\", (float('nan'), float('nan'))))\n",
        "        buf.write(f\"{name}: {val:.2f} (CI: [{lo:.2f}, {hi:.2f}])\\n\")\n",
        "\n",
        "    # if optimization_status is not None:\n",
        "    #     buf.write(\"\\nOptimization status:\\n\")\n",
        "    #     buf.write(str(optimization_status) + \"\\n\")\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(buf.getvalue())\n",
        "\n",
        "    return valid_percent, smatch_score\n",
        "\n",
        "\n",
        "# Example\n",
        "gold_amrs = [\n",
        "    \"(a / approve-01 :ARG0 (p / person))\",\n",
        "    \"(a / approve-01 :ARG0 (p / person))\",\n",
        "    \"(x / person :mod (b / big))\"\n",
        "]\n",
        "sys_amrs = [\n",
        "    \"(a / approve-01 :ARG0 (p / person))\",  # valid and not correct\n",
        "    \"(a / approve-01 :ARG1 (p / person))\",  # valid but not correct\n",
        "    \"(x / invalid\"  # invalid: unbalanced parentheses\n",
        "]\n",
        "\n",
        "valid_pct, smatchpp_score = validate_and_score(gold_amrs, sys_amrs)\n",
        "\n",
        "smatchpp_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3709290b",
      "metadata": {
        "id": "3709290b"
      },
      "outputs": [],
      "source": [
        "def get_golds(sample):\n",
        "    golds = []\n",
        "    for prompt, gold in tqdm(sample):\n",
        "        gold_parse = gold['content']\n",
        "        golds += [gold_parse]\n",
        "    return golds\n",
        "\n",
        "def get_amrs_from_model(stog, sample):\n",
        "    data = []\n",
        "\n",
        "    instr = \"Generate an Abstract Meaning Representation (AMR) graph for the following sentence: \"\n",
        "    gens = []\n",
        "    for prompt, gold in tqdm(sample):\n",
        "        sent = prompt['content'].replace(instr, \"\")\n",
        "        gold_parse = gold['content']\n",
        "        gen_parse = stog.parse_sents([sent])[0]\n",
        "        data.append({\n",
        "            'sentence': sent,\n",
        "            'gold_amr': gold_parse,\n",
        "            'generated_amr': gen_parse\n",
        "        })\n",
        "        gens += [gen_parse]\n",
        "\n",
        "    results_df = pd.DataFrame(data)\n",
        "    results_df.to_csv(\"res_parse.tsv\", sep=\"\\t\")\n",
        "    return gens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b633e9",
      "metadata": {
        "id": "33b633e9"
      },
      "outputs": [],
      "source": [
        "PRICING = {\n",
        "    \"o3-2025-04-16\": {\"input\": 2, \"output\": 8},\n",
        "    \"gpt-4o\": {\"input\": 2.5, \"output\": 10},\n",
        "    \"gpt-5-2025-08-07\": {\"input\": 1.25, \"output\": 10}\n",
        "}\n",
        "MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
        "QUERY_INSTRUCTION = \"Represent this sentence for searching relevant passages: \"\n",
        "\n",
        "\n",
        "def load_encoder(model_name):\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "# Caching\n",
        "_ENCODER = None\n",
        "\n",
        "def load_encoder(name=\"BAAI/bge-base-en-v1.5\", device=\"cuda\"):\n",
        "    global _ENCODER\n",
        "    if _ENCODER is None:\n",
        "        _ENCODER = SentenceTransformer(name, device=device)\n",
        "        _ENCODER.max_seq_length = 384\n",
        "    return _ENCODER\n",
        "\n",
        "def build_or_load_index(\n",
        "    sentences,\n",
        "    model,\n",
        "    cache_dir = \".rag_cache\",\n",
        "    cache_name = \"bge_base_en_v15_50k\",\n",
        "    force_rebuild = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    sentences: list of short strings\n",
        "    returns: (faiss_index, id2doc) where id2doc[i] = {\"text\": str}\n",
        "    \"\"\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    emb_path   = os.path.join(cache_dir, f\"{cache_name}.npy\")\n",
        "    meta_path  = os.path.join(cache_dir, f\"{cache_name}.meta.json\")\n",
        "    faiss_path = os.path.join(cache_dir, f\"{cache_name}.faiss\")\n",
        "\n",
        "    id2doc = [{\"text\": s} for s in sentences]\n",
        "\n",
        "    # Simple load-if-present cache\n",
        "    if (\n",
        "        not force_rebuild\n",
        "        and os.path.exists(emb_path)\n",
        "        and os.path.exists(meta_path)\n",
        "        and os.path.exists(faiss_path)\n",
        "    ):\n",
        "        with open(meta_path, \"r\") as f:\n",
        "            cached_meta = json.load(f)\n",
        "        if len(cached_meta) == len(id2doc):\n",
        "            index = faiss.read_index(faiss_path)\n",
        "            return index, cached_meta\n",
        "\n",
        "    # Encode all sentences\n",
        "    emb = model.encode(\n",
        "        [d[\"text\"] for d in id2doc],\n",
        "        batch_size=512,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=True,\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    np.save(emb_path, emb)\n",
        "    with open(meta_path, \"w\") as f:\n",
        "        json.dump(id2doc, f, ensure_ascii=False)\n",
        "\n",
        "    dim = emb.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(emb)\n",
        "    faiss.write_index(index, faiss_path)\n",
        "    return index, id2doc\n",
        "\n",
        "def retrieve_top_k(\n",
        "    query_text,\n",
        "    model,\n",
        "    index,\n",
        "    id2doc,\n",
        "    top_k = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    returns: list of {\"text\", \"score\"} sorted by similarity (cosine)\n",
        "    \"\"\"\n",
        "    q_text = QUERY_INSTRUCTION + query_text\n",
        "    q = model.encode([q_text], normalize_embeddings=True).astype(np.float32)\n",
        "    scores, idxs = index.search(q, top_k)\n",
        "    scores, idxs = scores[0], idxs[0]\n",
        "\n",
        "    results = []\n",
        "    for s, i in zip(scores, idxs):\n",
        "        if i < 0:\n",
        "            continue\n",
        "        results.append({\"text\": id2doc[int(i)][\"text\"], \"score\": float(s)})\n",
        "    return results\n",
        "\n",
        "\n",
        "def obtain_few_shot_examples(query, index, id2doc, model, top_k=10):\n",
        "    top = retrieve_top_k(query, model, index, id2doc, top_k=top_k)\n",
        "    top_texts = [r['text'] for r in top]\n",
        "    texts_and_amrs = [full_train[full_train_texts.index(text)] for text in top_texts]\n",
        "    return texts_and_amrs\n",
        "\n",
        "\n",
        "def get_amrs_from_chatgpt(sample, system_prompt_init, dir_path, model=\"gpt-3.5-turbo\", prepend_to_message=\"\", append_to_message=\"\", form_few_shot_prompt=False, **kwargs):\n",
        "    \"\"\"\n",
        "    Generate AMRs from ChatGPT for a given sample of sentences.\n",
        "    \"\"\"\n",
        "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    data, gens, stats = [], [], []\n",
        "\n",
        "    for prompt, gold in tqdm(sample):\n",
        "        user_message =  prepend_to_message + prompt['content'] + append_to_message\n",
        "        gold_parse = gold['content']\n",
        "        if form_few_shot_prompt:\n",
        "            print(\"Loading encoder...\")\n",
        "            model_faiss = load_encoder()\n",
        "            print(\"Building/loading index...\")\n",
        "            index, id2doc = build_or_load_index(full_train_texts, model_faiss)\n",
        "            query = prompt['content'].replace(\"Generate an Abstract Meaning Representation (AMR) graph for the following sentence: \", \"\")\n",
        "            few_shot_examples = obtain_few_shot_examples(query, index, id2doc, model_faiss)\n",
        "            formatted_examples = [f\"{item[0]['content']}\\nAMR graph:{item[1]['content']}\" for item in few_shot_examples]\n",
        "            system_prompt = system_prompt_init + \" Examples:\" + \"\\n\".join(formatted_examples)\n",
        "        else:\n",
        "            system_prompt = system_prompt_init\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_message}\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                **kwargs\n",
        "            )\n",
        "\n",
        "            gen_parse = response.choices[0].message.content.strip()\n",
        "\n",
        "            if gen_parse.startswith(\"```\"):\n",
        "                gen_parse = gen_parse.split(\"```\")[1]\n",
        "            if gen_parse.startswith(\"`\"):\n",
        "                gen_parse = gen_parse.split(\"`\")[1]\n",
        "            if gen_parse.startswith(\"amr\"):\n",
        "                gen_parse = gen_parse[3:].strip()\n",
        "\n",
        "            prompt_tokens = response.usage.prompt_tokens\n",
        "            completion_tokens = response.usage.completion_tokens\n",
        "            total_tokens  = response.usage.total_tokens\n",
        "            print(f\"total_tokens: {total_tokens}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sentence '{user_message}': {e}\")\n",
        "            gen_parse = 'API call failed'\n",
        "            prompt_tokens, completion_tokens, total_tokens = 0, 0, 0\n",
        "\n",
        "        data.append({\n",
        "            'sentence': user_message,\n",
        "            'gold_amr': gold_parse,\n",
        "            'generated_amr': gen_parse,\n",
        "        })\n",
        "        stats.append({\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens,\n",
        "            \"total_tokens\": total_tokens\n",
        "        })\n",
        "        gens.append(gen_parse)\n",
        "\n",
        "    prompt_tokens_final = sum(s.get(\"prompt_tokens\", 0) or 0 for s in stats)\n",
        "    completion_tokens_final = sum(s.get(\"completion_tokens\", 0) or 0 for s in stats)\n",
        "\n",
        "    final = {\n",
        "        \"prompt_tokens_final\": prompt_tokens_final,\n",
        "        \"completion_tokens_final\": completion_tokens_final,\n",
        "        \"total_tokens_final\": sum(s.get(\"total_tokens\", 0) or 0 for s in stats),\n",
        "        \"price\": (prompt_tokens_final * PRICING[model][\"input\"] + completion_tokens_final * PRICING[model][\"output\"]) / 1000000\n",
        "    }\n",
        "    stats.append(final)\n",
        "    results_df = pd.DataFrame(data)\n",
        "    results_df.to_csv(f\"{dir_path}/amr_parses.tsv\", sep=\"\\t\")\n",
        "\n",
        "    return stats, gens\n",
        "\n",
        "\n",
        "def generate_and_eval(\n",
        "        sample,\n",
        "        system_prompt, \n",
        "        model, \n",
        "        comment=\"test\", \n",
        "        prepend_to_message=\"\", \n",
        "        append_to_message=\"\", \n",
        "        form_few_shot_prompt=False, \n",
        "        **kwargs\n",
        "    ):\n",
        "    current_time = datetime.now()\n",
        "    filename_time = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    dir_path = f\"/content/gdrive/My Drive/amr_parsing/results/{model}-{filename_time}_{comment}\"\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    stats, gens = get_amrs_from_chatgpt(sample, system_prompt, dir_path, model=model, prepend_to_message=prepend_to_message, append_to_message=append_to_message, form_few_shot_prompt=form_few_shot_prompt, **kwargs)\n",
        "    with open(f\"{dir_path}/stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join([str(d) for d in stats]))\n",
        "    golds = get_golds(sample)\n",
        "    valid_pct, smatchpp_score = validate_and_score(golds, gens, report_path=f\"{dir_path}/evals.txt\")\n",
        "    return valid_pct, smatchpp_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbf2b7e6",
      "metadata": {
        "id": "bbf2b7e6"
      },
      "source": [
        "## Running inference and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44058904",
      "metadata": {
        "id": "44058904"
      },
      "source": [
        "Full results can be found in `results/` folder. Each run of each model has its own dedicated folder, formatted as `{model_name}-{run_timestamp}_{prompt_info}`. Each folder contains 1) `amr_parses.tsv` file with sentences, gold parses and generated parses and 2) `evals.txt` file with metrics and information on invalid graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c221a17b",
      "metadata": {},
      "source": [
        "### gpt-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcaa2fe5",
      "metadata": {
        "id": "dcaa2fe5"
      },
      "outputs": [],
      "source": [
        "model=\"gpt-4o\"\n",
        "\n",
        "system_prompt_baseline = \"\"\"You are an expert in Abstract Meaning Representation (AMR) parsing. Concept names must always have variables. Variables are lowercase letters with optional digits, e.g. (c1 / concept ...). Different concepts always get different variables, e.g. (c1 / concept ...) and (c2 / concept), even if the concept name is the same. To refer to the before-mentioned concept, you may use just the variable without brackets. Comments are absolutely not allowed. You only generate AMR parses and nothing else.\"\"\"\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt_baseline, model=model, comment=\"add_instr\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30481389",
      "metadata": {
        "id": "30481389"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"You are an expert in Abstract Meaning Representation (AMR) parsing. Concept names must always have variables. Variables are lowercase letters with optional digits, e.g. (c1 / concept ...). Different concepts always get different variables, e.g. (c1 / concept ...) and (c2 / concept), even if the concept name is the same. To refer to the before-mentioned concept, you may use just the variable without brackets. Comments are absolutely not allowed. You only generate AMR parses and nothing else.\"\"\"\n",
        "\n",
        "examples = [f\"{item[0]['content']}\\nAMR graph:{item[1]['content']}\" for item in sample_train]\n",
        "system_prompt_10_shot = system_prompt + \" Examples:\" + \"\\n\".join(examples)\n",
        "\n",
        "valid_pct_10_shot, smatchpp_score_10_shot = generate_and_eval(sample_test, system_prompt_10_shot, model=model, comment=\"10_shot\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165d4778",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"./AMR_detailed_instruction.txt\", 'r') as f:\n",
        "    system_prompt = f.read()\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt, model=model, comment=\"huge_prompt\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d520ad7c",
      "metadata": {},
      "source": [
        "### o3-2025-04-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd551c03",
      "metadata": {
        "id": "bd551c03"
      },
      "outputs": [],
      "source": [
        "model=\"o3-2025-04-16\"\n",
        "system_prompt_baseline = \"\"\"You are an expert in Abstract Meaning Representation (AMR) parsing. Concept names must always have variables. Variables are lowercase letters with optional digits, e.g. (c1 / concept ...). Different concepts always get different variables, e.g. (c1 / concept ...) and (c2 / concept), even if the concept name is the same. To refer to the before-mentioned concept, you may use just the variable without brackets. Comments are absolutely not allowed. You only generate AMR parses and nothing else.\"\"\"\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt_baseline, model=model, comment=\"add_instr\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d149fd5e",
      "metadata": {
        "id": "d149fd5e"
      },
      "outputs": [],
      "source": [
        "examples = [f\"{item[0]['content']}\\nAMR graph:{item[1]['content']}\" for item in sample_train]\n",
        "system_prompt_10_shot = system_prompt_baseline + \" Examples:\" + \"\\n\".join(examples)\n",
        "\n",
        "valid_pct_10_shot, smatchpp_score_10_shot = generate_and_eval(sample_test, system_prompt_10_shot, model=model, comment=\"10_shot\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aca1db7",
      "metadata": {
        "id": "7aca1db7"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/AMR_detailed_instruction.txt\", 'r') as f:\n",
        "    system_prompt = f.read()\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt, model=model, comment=\"huge_prompt\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a29d4d",
      "metadata": {},
      "source": [
        "### gpt-5-2025-08-07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da9ad2b",
      "metadata": {
        "id": "0da9ad2b"
      },
      "outputs": [],
      "source": [
        "model=\"gpt-5-2025-08-07\"\n",
        "system_prompt_baseline = \"\"\"You are an expert in Abstract Meaning Representation (AMR) parsing. Concept names must always have variables. Variables are lowercase letters with optional digits, e.g. (c1 / concept ...). Different concepts always get different variables, e.g. (c1 / concept ...) and (c2 / concept), even if the concept name is the same. To refer to the before-mentioned concept, you may use just the variable without brackets. Comments are absolutely not allowed. You only generate AMR parses and nothing else.\"\"\"\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt_baseline, model=model, comment=\"add_instr\", append_to_message=\"\\nAMR graph:\", max_completion_tokens=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf3328f",
      "metadata": {
        "id": "ccf3328f"
      },
      "outputs": [],
      "source": [
        "examples = [f\"{item[0]['content']}\\nAMR graph:{item[1]['content']}\" for item in sample_train]\n",
        "system_prompt_10_shot = system_prompt_baseline + \" Examples:\" + \"\\n\".join(examples)\n",
        "\n",
        "valid_pct_10_shot, smatchpp_score_10_shot = generate_and_eval(sample_test, system_prompt_10_shot, model=model, comment=\"10_shot\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1347ce8",
      "metadata": {
        "id": "b1347ce8"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/AMR_detailed_instruction.txt\", 'r') as f:\n",
        "    system_prompt = f.read()\n",
        "\n",
        "valid_pct, smatchpp_score = generate_and_eval(sample_test, system_prompt, model=model, comment=\"huge_prompt\", append_to_message=\"\\nAMR graph:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35421d2",
      "metadata": {
        "id": "e35421d2"
      },
      "source": [
        "## Offline parse evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df44bb70",
      "metadata": {
        "id": "df44bb70"
      },
      "source": [
        "If the parses were saved successfully to `{some_folder}/amr_parses.tsv`, but were not evaluated, we can conduct evaluation post-hoc using the following function.\n",
        "\n",
        "Also, here are the functions that let you get the best and the worst AMR parses and visualise them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "0adc77d8",
      "metadata": {
        "id": "0adc77d8"
      },
      "outputs": [],
      "source": [
        "from amrlib.graph_processing.amr_plot import AMRPlot\n",
        "\n",
        "\n",
        "def score_file(filepath, draft=False):\n",
        "    df = pd.read_csv(filepath, sep='\\t')\n",
        "\n",
        "    golds = df['gold_amr'].tolist()\n",
        "    gens = df['generated_amr'].tolist()\n",
        "    if draft:\n",
        "        gens = df['draft_amr'].tolist()\n",
        "\n",
        "\n",
        "    gens_clean = [\"(\" + \"(\".join(text.split(\"(\")[1:]).replace(\"AMR graph:\", \"\") for text in gens]\n",
        "\n",
        "    valid_pct, smatchpp_score = validate_and_score(golds, gens_clean)\n",
        "    print(f\"Valid AMRs: {valid_pct:.1f}%\")\n",
        "    print(f\"Smatch++ score: {smatchpp_score}\")\n",
        "    return valid_pct, smatchpp_score\n",
        "\n",
        "\n",
        "def get_sorted_amr_from_file(filepath, draft=False):\n",
        "    \"\"\"\n",
        "    Returns a list of amr tuples sorted by F1 score.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filepath, sep=\"\\t\")\n",
        "    sents =  df[\"sentence\"].tolist()\n",
        "    golds = df[\"gold_amr\"].tolist()\n",
        "    gens = (df[\"draft_amr\"] if draft else df[\"generated_amr\"]).tolist()\n",
        "    gens_clean = [\"(\" + \"(\".join(text.split(\"(\")[1:]).replace(\"AMR graph:\", \"\") for text in gens]\n",
        "\n",
        "    scored = []\n",
        "    for i, (gold, gen, sent) in enumerate(zip(golds, gens_clean, sents)):\n",
        "        valid_pct, smatchpp_score = validate_and_score([gold], [gen])\n",
        "        f1 = smatchpp_score.get(\"main\", {}).get(\"F1\", {}).get(\"result\", None)\n",
        "        if f1 is None:\n",
        "            f1 = 0.0\n",
        "        scored.append((float(f1), sent, gen, gold, i))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "\n",
        "    return scored\n",
        "\n",
        "\n",
        "def visualise_amr(graph, save_to):\n",
        "    plot = AMRPlot(render_fn=f\"{save_to}.gv\", format=\"pdf\")\n",
        "    plot.build_from_graph(graph, debug=False)\n",
        "    plot.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05ee275e",
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted_amrs = get_sorted_amr_from_file('./results/fin/gpt-5-2025-08-07-2025-09-29_21-27-34_langchain/amr_parses.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "1aa92074",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: 09/02/2010 13:25\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: 2008-10-08\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Crime; weapons; international; money\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Xinhua News Agency , Seoul , August 31st , by reporter Shuifu Tang\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: 2007-06-18\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Senior Fellow at the International Institute for Strategic Studies mark Fitzpatrick stated that --\n",
            "100.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: 08/02/2010 13:52\n",
            "97.14 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Premier Peng Li and Kirghizian President Akayev attended the signing ceremony .\n",
            "96.88 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Iran is a signatory to the Nuclear Nonproliferation Treaty but not a member of the Nuclear Suppliers Group.\n"
          ]
        }
      ],
      "source": [
        "# get BEST AMR parses; plots saves to `pics/` folder\n",
        "\n",
        "for i in range(1, 10):\n",
        "    print(sorted_amrs[-i][0], sorted_amrs[-i][1])\n",
        "    visualise_amr(sorted_amrs[i][2], f\"pics/best/{i}_gen\")\n",
        "    visualise_amr(sorted_amrs[i][3], f\"pics/best/{i}_gold\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "e627a834",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: m1456\n",
            "46.81 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Maritime officials in Kenya stated that critical details have yet to be agreed upon.\n",
            "48.78 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: The list was made available on condition that neither the diplomat nor the diplomat's country be identified.\n",
            "49.12 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Some of these flights were innocent violations by ranchers in the Amazon flying between plantations.\n",
            "50.0 Generate an Abstract Meaning Representation (AMR) graph for the following sentence: SUPPLIES WE NEED: comfortable nursing chairs and rockers, nursing foot stool, boppie, SLINGS and baby clothes, diapers.\n"
          ]
        }
      ],
      "source": [
        "# get WORST AMR parses; plots saves to `pics/` folder\n",
        "\n",
        "for i in range(5):\n",
        "    print(sorted_amrs[i][0], sorted_amrs[i][1])\n",
        "    visualise_amr(sorted_amrs[i][2], f\"pics/worst/{i}_gen\")\n",
        "    visualise_amr(sorted_amrs[i][3], f\"pics/worst/{i}_gold\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3cdedc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3cdedc",
        "outputId": "a0025764-730c-4d79-9b27-368a74b3f044"
      },
      "outputs": [],
      "source": [
        "score_file('./results/fin/gpt-5-2025-08-07-2025-09-29_21-27-34_langchain/amr_parses.tsv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HBImZG6DYYxh",
      "metadata": {
        "id": "HBImZG6DYYxh"
      },
      "source": [
        "# LangChain self-correcting agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b00129",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zaVxuCKUYlhk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaVxuCKUYlhk",
        "outputId": "247676ff-9c22-40c8-ef72-ad50ef7bcc45"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
        "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from pydantic import ValidationError,  BaseModel, Field\n",
        "import json\n",
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "from typing import TypedDict, Optional, Union\n",
        "from pydantic import BaseModel, Field\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "text_llm_json = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a17d8f7",
      "metadata": {},
      "source": [
        "### Response classes (define the form of the model output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIGIlbj2f-_L",
      "metadata": {
        "id": "dIGIlbj2f-_L"
      },
      "outputs": [],
      "source": [
        "class GenerateAMR(BaseModel):\n",
        "    \"\"\"Generate a valid AMR.\"\"\"\n",
        "    amr: str = Field(description=\"Valid AMR graph.\")\n",
        "\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    score: int = Field(description=\"Score out of 5. 0 is for invalid or completely incorrect AMRs, 5 is for perfectly valid and sematically correct AMRs.\")\n",
        "    semantic_correctness: Union[str, None] = Field(description=\"Critique of AMR semantic correctness, e.g. roles, if any. Can be None.\")\n",
        "    structure: Union[str, None] = Field(description=\"Critique of AMR structure, if any. Can be None.\")\n",
        "\n",
        "\n",
        "class ReviseAnswer(BaseModel):\n",
        "    \"\"\"Revise your original answer to your question. Provide a well-formed, semantically faithful AMR and nothing else.\"\"\"\n",
        "\n",
        "    final_amr: str = Field(\n",
        "        description=\"Well-formed, semantically faithful AMR, improved based on feedback.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class AutoEvalOut(BaseModel):\n",
        "    score: Union[str, int, float] = Field(...)\n",
        "    comments: str = Field(..., min_length=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bdca44",
      "metadata": {},
      "source": [
        "### Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "auFXEvB1Z19s",
      "metadata": {
        "id": "auFXEvB1Z19s"
      },
      "outputs": [],
      "source": [
        "amr_system_prompt = \"\"\"Rules of Abstract Meaning Representation (AMR) parsing. Concept names must always have variables. Variables are lowercase letters with optional digits, e.g. (c1 / concept ...). Different concepts always get different variables, e.g. (c1 / concept ...) and (c2 / concept), even if the concept name is the same. To refer to the before-mentioned concept, just the variable without brackets may be used.\"\"\"\n",
        "\n",
        "with open(\"./data/AMR_detailed_instruction.txt\", 'r') as f:\n",
        "    amr_detailed_prompt = f.read()\n",
        "\n",
        "draft_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", amr_system_prompt),\n",
        "    (\"user\", \"\"\"Generate AMR parse for the following sentence: {question}\n",
        "\n",
        "Here are examples of correct AMR parses: {examples}\n",
        "Comments are absolutely not allowed. You only generate AMR parses and nothing else.\"\"\")\n",
        "])\n",
        "\n",
        "reflect_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", f\"AMR parsong rules: {amr_detailed_prompt}\"),\n",
        "    (\"user\", \"\"\"Here are examples of correct AMR parses: {examples}\n",
        "\n",
        "Sentence: {question}\n",
        "AMR draft: {draft}\n",
        "\n",
        "Results of automatic evaluation: {auto_eval_comments}\n",
        "\n",
        "Provide a score (from 0 to 5) for the given AMR draft and very concise critique of crucial errors in structure and semantic correctness if any.\"\"\")\n",
        "])\n",
        "\n",
        "revise_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", amr_system_prompt),\n",
        "    (\"user\", \"\"\"Here are examples of correct AMR parses: {examples}\n",
        "\n",
        "Sentence: {question}\n",
        "AMR draft: {draft}\n",
        "\n",
        "Results of automatic evaluation: {auto_eval_comments}\n",
        "Feedback:\n",
        "{feedback}\n",
        "\n",
        "Revise your previous answer using the new information.\n",
        "- It is crucially important to fix issues reported in automatic evaluation, if any.\n",
        "- You should use the previous critique to ensure that AMR is well-formed.\n",
        "- You should use the previous critique to ensure that AMR is semantically faithful.\n",
        "Provide the new correct AMR and absolutely nothing else. No comments allowed. Prioritise making AMR well-formed according to Results of automatic evaluation, if any.\"\"\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KOSFnCG3a-HU",
      "metadata": {
        "id": "KOSFnCG3a-HU"
      },
      "outputs": [],
      "source": [
        "draft_model = text_llm_json.with_structured_output(GenerateAMR)\n",
        "reflect_model = text_llm_json.with_structured_output(Reflection)\n",
        "revise_model = text_llm_json.with_structured_output(ReviseAnswer)\n",
        "\n",
        "draft_chain   = draft_prompt   | draft_model\n",
        "reflect_chain = reflect_prompt | reflect_model\n",
        "revise_chain  = revise_prompt  | revise_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0247f3",
      "metadata": {},
      "source": [
        "### State, nodes, token counting helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eXRbZ4x6fTjv",
      "metadata": {
        "id": "eXRbZ4x6fTjv"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict, total=False):\n",
        "    question: str\n",
        "    draft: str\n",
        "    auto_eval_comments: str\n",
        "    feedback: str\n",
        "    final_amr: str\n",
        "    score: Union[str, int]\n",
        "    semantic_correctness: str\n",
        "    structure: str\n",
        "    examples: str\n",
        "    prompt_tokens_draft: Union[str, int, float]\n",
        "    completion_tokens_draft: Union[str, int, float]\n",
        "    total_tokens_draft: Union[str, int, float]\n",
        "    prompt_tokens_reflect: Union[str, int, float]\n",
        "    completion_tokens_reflect: Union[str, int, float]\n",
        "    total_tokens_reflect: Union[str, int, float]\n",
        "    prompt_tokens_revise: Union[str, int, float]\n",
        "    completion_tokens_revise: Union[str, int, float]\n",
        "    total_tokens_revise: Union[str, int, float]\n",
        "    draft_obj: GenerateAMR\n",
        "    reflect_obj: Reflection\n",
        "    revise_obj: ReviseAnswer\n",
        "    revise_count: int\n",
        "\n",
        "\n",
        "class TokenCounter(BaseCallbackHandler):\n",
        "    def __init__(self):\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def on_llm_end(self, response, **kwargs):\n",
        "        llm_output = getattr(response, \"llm_output\", {}) or {}\n",
        "        usage = llm_output.get(\"token_usage\", {}) or {}\n",
        "        # OpenAI-style keys; fallbacks for other providers\n",
        "        pt = usage.get(\"prompt_tokens\") or usage.get(\"input_tokens\") or 0\n",
        "        ct = usage.get(\"completion_tokens\") or usage.get(\"output_tokens\") or 0\n",
        "        tt = usage.get(\"total_tokens\") or (pt + ct)\n",
        "        self.prompt_tokens += int(pt)\n",
        "        self.completion_tokens += int(ct)\n",
        "        self.total_tokens += int(tt)\n",
        "\n",
        "\n",
        "def draft_node(state):\n",
        "    print(\"Loading encoder...\")\n",
        "    model_faiss = load_encoder()\n",
        "    print(\"Building/loading index...\")\n",
        "    index, id2doc = build_or_load_index(full_train_texts, model_faiss)\n",
        "    query = state[\"question\"].replace(\"Generate an Abstract Meaning Representation (AMR) graph for the following sentence: \", \"\")\n",
        "    few_shot_examples = obtain_few_shot_examples(query, index, id2doc, model_faiss)\n",
        "    formatted_examples = [f\"{item[0]['content']}\\nAMR graph:{item[1]['content']}\" for item in few_shot_examples]\n",
        "    tc = TokenCounter()\n",
        "    obj: GenerateAMR = draft_chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"examples\": formatted_examples\n",
        "    }, config={\"callbacks\": [tc]})\n",
        "    return {\n",
        "        \"draft_obj\": obj,\n",
        "        \"draft\": obj.amr,\n",
        "        \"examples\": formatted_examples,\n",
        "        \"prompt_tokens_draft\": tc.prompt_tokens,\n",
        "        \"completion_tokens_draft\": tc.completion_tokens,\n",
        "        \"total_tokens_draft\": tc.total_tokens\n",
        "        }\n",
        "\n",
        "def reflect_node(state: State) -> State:\n",
        "    tc = TokenCounter()\n",
        "    obj: Reflection = reflect_chain.invoke({\n",
        "        \"question\": state[\"question\"],\n",
        "        \"examples\": state[\"examples\"],\n",
        "        \"draft\": state[\"draft\"],\n",
        "        \"auto_eval\": state[\"score\"],\n",
        "        \"auto_eval_comments\": state[\"auto_eval_comments\"]\n",
        "    }, config={\"callbacks\": [tc]})\n",
        "    return {\n",
        "        \"reflect_obj\": obj,\n",
        "        \"score\": obj.score,\n",
        "        \"feedback\": f\"Score: {obj.score} Semantic_correctness: {obj.semantic_correctness} Structure: {obj.structure}\",\n",
        "        \"prompt_tokens_reflect\": tc.prompt_tokens,\n",
        "        \"completion_tokens_reflect\": tc.completion_tokens,\n",
        "        \"total_tokens_reflect\": tc.total_tokens\n",
        "        }\n",
        "\n",
        "def revise_node(state: State) -> State:\n",
        "    c = state.get(\"revise_count\", 0) + 1\n",
        "    if int(state.get(\"score\", 0)) == 5:\n",
        "        return {\n",
        "            \"revise_count\": c,\n",
        "            \"final_amr\": state[\"draft\"]\n",
        "            }\n",
        "\n",
        "    else:\n",
        "        tc = TokenCounter()\n",
        "        obj: ReviseAnswer = revise_chain.invoke({\n",
        "            \"question\": state[\"question\"],\n",
        "            \"draft\": state[\"draft\"],\n",
        "            \"feedback\": state[\"feedback\"],\n",
        "            \"examples\": state[\"examples\"],\n",
        "            \"auto_eval_comments\": state[\"auto_eval_comments\"]\n",
        "        }, config={\"callbacks\": [tc]})\n",
        "        return {\n",
        "            \"revise_obj\": obj,\n",
        "            \"revise_count\": c,\n",
        "            \"final_amr\": obj.final_amr,\n",
        "            \"prompt_tokens_revise\": tc.prompt_tokens,\n",
        "            \"completion_tokens_revise\": tc.completion_tokens,\n",
        "            \"total_tokens_revise\": tc.total_tokens\n",
        "        }\n",
        "\n",
        "\n",
        "def auto_eval(amr):\n",
        "    \"\"\"Evaluate generated AMRs (PENMAN and basic graph validation)\"\"\"\n",
        "    ok, comment = is_valid_amr(amr)\n",
        "    if not ok:\n",
        "        score = 0\n",
        "        comment = f\"\\nAMR is invalid: {comment}\"\n",
        "    else:\n",
        "        score = \"to be determined\"\n",
        "        comment = \"No structural errors found.\"\n",
        "    return AutoEvalOut(score=score, comments=comment)\n",
        "\n",
        "\n",
        "def auto_eval_node(state: State) -> State:\n",
        "    if state.get(\"final_amr\"):\n",
        "        result = auto_eval(state.get(\"final_amr\"))\n",
        "        result_dict = {\n",
        "            \"draft\": state.get(\"final_amr\"),\n",
        "            \"old_draft\": state.get(\"draft\")\n",
        "        }\n",
        "    else:\n",
        "        result = auto_eval(state.get(\"draft\"))\n",
        "        result_dict = {}\n",
        "    result_dict[\"auto_eval\"] = result.model_dump()\n",
        "    result_dict[\"score\"] = result.score\n",
        "    result_dict[\"auto_eval_comments\"] = result.comments\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205228f7",
      "metadata": {},
      "source": [
        "### Building the final LangChain graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2fLSpYXgG-W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585,
          "referenced_widgets": [
            "1fdfb6efc313475b9a500ea45f20e445",
            "7b17bd2e7c6746978064646019c8df41",
            "e6d49e8e4cd045408891675a691eb798",
            "6a62b9a6384445069938cdd03221055d",
            "aa723ebe615d40ff8b71acf1f4c0a4b6",
            "9a8d7f4833ad401d87662c71a03c5434",
            "1f91c500217e4e668bafe534a8a0be4e",
            "a0649ef3308a465f8d689c2d61132765",
            "4c1f57233d574ea8a2d30a510ad260d5",
            "efecbcef27a04ad6a2d90fde107b831c",
            "2e8371d9a93a46cc8bf67fee544baf51",
            "627202f2e860470b82a4de6eb9054dbf",
            "dab05b718e194374b487fe1c3fca6f77",
            "a3f7396b61f04da7a6bf4fd594ab35f2",
            "e3e36fa045234247b3bfee1f23b3a61c",
            "139bae0315c942409a6c40edf1c00b94",
            "3fa3d5c756194821a0cdd9aa179689d0",
            "3f7b3616d2cc47f899bee8dbb059baed",
            "5963490ce569422bb4619aafaef68b29",
            "7def10a081d74560865fb2a5ab76641c",
            "85f2e3fadd6c46ea8336d1353a186fa2",
            "964e0d25e3dc4a8fbd6a672eb3cd16eb",
            "b4b3bb6fcd174b5e953819fe00c01dc5",
            "ed6a301056854d4586b5750311733955",
            "d7b4b725adcf4a85983ebc0a704086c0",
            "7cb104d6bc1a475bb22d8edfcb3c90a3",
            "d5aea041a7fe4e8eb3626094790aebb3",
            "7515dc34d66d4814b212141ba07856ed",
            "7e8473f6516c4ade92a6d9c8c7f37de0",
            "20333ffad481450392b1ccc5b04d9ac5",
            "3c8a8b9b91a646b3914ebcb8d804e93f",
            "a77ef1de837d44769bffefc043ec2ed9",
            "f34661d2e8f9432e85122282c373c776",
            "184040022f424d8aa8bf7863de848620",
            "c272e7f0d77f4b8288d130e03b9de42a",
            "9fc1654ba15d4e8c9b3316ea00fd1312",
            "a74809a6ce434420bd66ab21f1d575ed",
            "b54ab64efd3c44e2b691763312d72847",
            "2af852c52dd340bea03f82f0525da654",
            "ca6b7c1ebf784c7cb60394079f6139f6",
            "526455671f2d4a93b324bbabd68298db",
            "4df90b27c4974867b4b54e8b3b0321f5",
            "804f32cc8c8c4b4cbeeb92b9c0b1a59e",
            "d8a9edbd82b344369ef0102b56a0497a",
            "f39774b2e76a43689ee6a91fe34deceb",
            "db544e182ffb4d299c944281605f33c0",
            "388578ab788b4777bc6f2a73d73e98ca",
            "23eea94e897a45e38e89894929ceecda",
            "44a2bc55524041cf8526653c49f86f4d",
            "521982a4cbe5470ba1f340474b59364b",
            "36c85328f00a40cfb110fea562bb2713",
            "41e3b65bbfda4838abe200a66c0d78be",
            "33e49c24c1574e809f75b52e0efe8652",
            "0db39167df754f629d3f18bedbaae4fb",
            "35850d307f86442ab0a416c4f3da8878",
            "b07b39c6edfc44fe97716755d571b638",
            "76e7638d2be5431d8cbd8da112177c8f",
            "5a5999147b524e978dee820ec5110b79",
            "89b02b8a23854ee39d4d534daa6d9629",
            "3a824129314142bc82325690f9ee2d3a",
            "ef496e9c7416445d8f7cec353ee65686",
            "f13191ea319343aeb17202012d91dc29",
            "d4c7a0dc939d42499bfe72c5d76a98e7",
            "4b6dbe1165634dd4a3d51255a023cd91",
            "d431db6c244649bba0e83adbc59b9359",
            "7dccc94c78814ab7913307a822891d89",
            "9da46d5c958b4411824f0fcda236b659",
            "d5a15330feed4f98abda07e7b2fff8cb",
            "68ed93c405dc4b4a89f9629d2f670fe7",
            "3c0b6826e19a4a4fb641af61e41c4694",
            "e8792427ebfd4e209561b87eaa3b50b0",
            "b7e7b5a5e90e4e0ab6a2a222c31e750a",
            "ea185989c5a340a7a2d2e5292a359d10",
            "63ba52dbe01a4b38853d3333667015e8",
            "f64a71c638b7419a92dc71d2fc5ed785",
            "bb400586926843a2a1c6ee288e395204",
            "9ef8d22208e243aaaa6c7105c24b576e",
            "eaddf5002387484182a793a865e2a7ba",
            "96228b2d9bc3438b810730e426c176ff",
            "730bf35baaa440d08f5b111ecd8b602b",
            "96a6127832024d1f9e30648c346d7721",
            "b4a8cfae7a4d4af3aecaff80b130d75b",
            "54ff9dbf925b479ab4cc2f348f200b75",
            "70ce33abbad8420fa99a38840e24a5bb",
            "ffff918d3f684f2987639ad7bd3fe4ce",
            "768c4cccbb134bd4873b35ce0552e516",
            "031391147b3c48c29002fc1ab1ed323a",
            "c3caed17e7514dbeba6c028a8119d14d",
            "99e0036f39964afb824d10e03a72a214",
            "be4d72a1a4ee4ad78ff9db6aec32b7dd",
            "ba79505fdaeb42d084cf39565c9fda98",
            "358472f519ce4530990dcf973616054e",
            "66e3ec4229414af78c770c582fa7577f",
            "e84baba12d794ba4baa428564dce60d9",
            "564bcb3e38fb47b98a983b42c61ff421",
            "bc0f6e62b6534bd48a482f58d2fa130f",
            "565f5cc23e9345a4992df97b6a3fcd88",
            "3871eb7d68244aee8fa7fdb0206817d1",
            "0ba8bc65c1a94171b37bd224ae2d06c3",
            "03a9de28bcb04958a9dd84b94ebd7d86",
            "5e8fc5efbb1342c0b433f5488ad22882",
            "908c0ca0bd824607aba9e021a51c4659",
            "4bd8ccccdfde4328be07b93c9fa909f0",
            "9f7cbd6bada64dce9e0a714c032ba50c",
            "9d9ac1ee7c2f4b6eab52df2be9e3c577",
            "cb9d99c84d0a4b7da90d4c1958300ec6",
            "861aad28b4c74243ac8630ca03c28f1a",
            "6c2007d6e15748b5a3ee7a2b49acad3c",
            "0af9e5ddfb3d4f68a575ef3e278dfa38",
            "40c5f1a4d88645ea8e8b5726c75d70c6",
            "d6a49772877b4e7e8c9c65cb0add0b39",
            "3f137601c4d04da7b31b459c3b04a9ed",
            "5271446e55ff4df59d4ba0355a3f0ffc",
            "3104161e1f294a17800de82af7d9f9c5",
            "322070fc92404d21954dab91f226e08d",
            "236b67a203cd4a61a45670696f7c8ab8",
            "9042edefbb6143269bd78ffb12ecf203",
            "46ebf4b49bb74045968c468cc55af080",
            "4b854c1f2f5a422da7a9ce399d83084b",
            "9b4097155339415a8f7c8437530a1850",
            "45f785ce27b84e6d9fa0cc2f9645228c",
            "a8d97367872e4cfca3151a66a24e431a",
            "0d9d31ed91e644fda871574e5ea09ea4",
            "fd6950b8fd604a5a9bf0cb1f8310c9dd",
            "4ac33e6137f94ee1982ac7485cd64eab",
            "563143ae8727422fa1497e2f2c107d44",
            "30282b2a7a5345fabc6885248a90a8b2",
            "447ea66cacd84e21bafb7ed9af467a06",
            "116d052508314342be26d1264c92ea6f",
            "e0351fa193e94f558f0c869bdce64297",
            "c79272191a93455d86493be06650a24f",
            "7269a00ea63d402182d12d580fe64b99"
          ]
        },
        "id": "d2fLSpYXgG-W",
        "outputId": "95077315-da75-4dcf-f383-9a2f40b7b279"
      },
      "outputs": [],
      "source": [
        "def route_after_revise(state):\n",
        "    result = auto_eval(state.get(\"final_amr\"))\n",
        "    c = state.get(\"revise_count\", 0)\n",
        "\n",
        "    return \"eval\" if result.score == 0 and c <= 3 else \"end\"\n",
        "\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"draft\", draft_node)\n",
        "builder.add_node(\"eval\", auto_eval_node)\n",
        "builder.add_node(\"reflect\", reflect_node)\n",
        "builder.add_node(\"revise\", revise_node)\n",
        "\n",
        "builder.add_edge(START, \"draft\")\n",
        "builder.add_edge(\"draft\", \"eval\")\n",
        "builder.add_edge(\"eval\", \"reflect\")\n",
        "builder.add_edge(\"reflect\", \"revise\")\n",
        "builder.add_conditional_edges(\n",
        "    \"revise\",\n",
        "    route_after_revise,\n",
        "    {\"end\": END, \"eval\": \"eval\"}\n",
        ")\n",
        "\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0kQVvLENgbAh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kQVvLENgbAh",
        "outputId": "9d182c4d-fc0f-4c99-cf05-b109eaac4c6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Thank you',\n",
              " 'draft': '(t / thank-01\\n      :ARG1 (y / you))',\n",
              " 'auto_eval_comments': 'No structural errors found.',\n",
              " 'feedback': 'Score: 5 Semantic_correctness: None Structure: None',\n",
              " 'final_amr': '(t / thank-01\\n      :ARG1 (y / you))',\n",
              " 'score': 5,\n",
              " 'examples': ['Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thank you\\nAMR graph:(s / say-01\\n      :ARG1 (t / thank-01\\n            :ARG1 (y / you))\\n      :ARG2 y)',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thank you\\nAMR graph:(s / say-01\\n      :ARG1 (t / thank-01\\n            :ARG1 (y / you))\\n      :ARG2 y)',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: thank you\\nAMR graph:(t / thank-01\\n      :ARG0 (i / i)\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thanking You.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: , thank you.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: (Thanks.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thankies.\\nAMR graph:(t / thank-01)',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thank you.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: thank you.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))',\n",
              "  'Generate an Abstract Meaning Representation (AMR) graph for the following sentence: Thank you.\\nAMR graph:(t / thank-01\\n      :ARG1 (y / you))'],\n",
              " 'prompt_tokens_draft': 623,\n",
              " 'completion_tokens_draft': 21,\n",
              " 'total_tokens_draft': 644,\n",
              " 'prompt_tokens_reflect': 13533,\n",
              " 'completion_tokens_reflect': 15,\n",
              " 'total_tokens_reflect': 13548,\n",
              " 'draft_obj': GenerateAMR(amr='(t / thank-01\\n      :ARG1 (y / you))'),\n",
              " 'reflect_obj': Reflection(score=5, semantic_correctness=None, structure=None),\n",
              " 'revise_count': 1}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test example to demonstrate the output format\n",
        "\n",
        "out = graph.invoke({\"question\": \"Thank you\"})\n",
        "\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d84d41",
      "metadata": {},
      "source": [
        "### Running LangChain agent on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "woLkIe2tC2AG",
      "metadata": {
        "id": "woLkIe2tC2AG"
      },
      "outputs": [],
      "source": [
        "def get_amrs_from_langchain(sample, model, dir_path, prepend_to_message=\"\", append_to_message=\"\", **kwargs):\n",
        "    \"\"\"\n",
        "    Generate AMRs from ChatGPT for a given sample of sentences.\n",
        "    \"\"\"\n",
        "    data, gens, stats, drafts, full_logs = [], [], [], [], []\n",
        "\n",
        "    for prompt, gold in tqdm(sample):\n",
        "        user_message =  prepend_to_message + prompt['content'] + append_to_message\n",
        "        gold_parse = gold['content']\n",
        "        out = None\n",
        "        try:\n",
        "\n",
        "            out = graph.invoke({\"question\": user_message})\n",
        "\n",
        "            gen_parse = out[\"final_amr\"]\n",
        "\n",
        "            if gen_parse.startswith(\"```\"):\n",
        "                gen_parse = gen_parse.split(\"```\")[1]\n",
        "            if gen_parse.startswith(\"`\"):\n",
        "                gen_parse = gen_parse.split(\"`\")[1]\n",
        "            if gen_parse.startswith(\"amr\"):\n",
        "                gen_parse = gen_parse[3:].strip()\n",
        "\n",
        "            prompt_tokens_draft = out.get(\"prompt_tokens_draft\", 0)\n",
        "            completion_tokens_draft = out.get(\"completion_tokens_draft\", 0)\n",
        "            total_tokens_draft = out.get(\"total_tokens_draft\", 0)\n",
        "            prompt_tokens_reflect = out.get(\"prompt_tokens_reflect\", 0)\n",
        "            completion_tokens_reflect = out.get(\"completion_tokens_reflect\", 0)\n",
        "            total_tokens_reflect = out.get(\"total_tokens_reflect\", 0)\n",
        "            prompt_tokens_revise = out.get(\"prompt_tokens_revise\", 0)\n",
        "            completion_tokens_revise = out.get(\"completion_tokens_revise\", 0)\n",
        "            total_tokens_revise = out.get(\"total_tokens_revise\", 0)\n",
        "            print(f\"total_tokens_draft, total_tokens_reflect, total_tokens_revise: {total_tokens_draft}, {total_tokens_reflect}, {total_tokens_revise}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing sentence '{user_message}': {e}\")\n",
        "            gen_parse = 'API call failed'\n",
        "            prompt_tokens_draft, completion_tokens_draft, total_tokens_draft = 0, 0, 0\n",
        "            prompt_tokens_reflect, completion_tokens_reflect, total_tokens_reflect = 0, 0, 0\n",
        "            prompt_tokens_revise, completion_tokens_revise, total_tokens_revise = 0, 0, 0\n",
        "\n",
        "\n",
        "        data_entry = {\n",
        "            'sentence': user_message,\n",
        "            'gold_amr': gold_parse,\n",
        "            'generated_amr': gen_parse,\n",
        "        }\n",
        "        if out and \"draft\" in out:\n",
        "            data_entry['draft_amr'] = out[\"draft\"]\n",
        "        else:\n",
        "            data_entry['draft_amr'] = \"N/A\"\n",
        "\n",
        "\n",
        "        data.append(data_entry)\n",
        "\n",
        "        stats.append({\n",
        "            \"prompt_tokens_draft\": prompt_tokens_draft,\n",
        "            \"completion_tokens_draft\": completion_tokens_draft,\n",
        "            \"total_tokens_draft\": total_tokens_draft,\n",
        "\n",
        "            \"prompt_tokens_reflect\": prompt_tokens_reflect,\n",
        "            \"completion_tokens_reflect\": completion_tokens_reflect,\n",
        "            \"total_tokens_reflect\": total_tokens_reflect,\n",
        "\n",
        "            \"prompt_tokens_revise\": prompt_tokens_revise,\n",
        "            \"completion_tokens_revise\": completion_tokens_revise,\n",
        "            \"total_tokens_revise\": total_tokens_revise\n",
        "        })\n",
        "        gens.append(gen_parse)\n",
        "        if out and \"draft\" in out:\n",
        "            drafts.append(out[\"draft\"])\n",
        "        else:\n",
        "            drafts.append(\"N/A\")\n",
        "\n",
        "        full_logs.append(out)\n",
        "\n",
        "    prompt_tokens_final = sum([s.get(\"prompt_tokens_draft\", 0) or 0 for s in stats] + [s.get(\"prompt_tokens_reflect\", 0) or 0 for s in stats] + [s.get(\"prompt_tokens_revise\", 0) or 0 for s in stats])\n",
        "    completion_tokens_final = sum([s.get(\"completion_tokens_draft\", 0) or 0 for s in stats] + [s.get(\"completion_tokens_reflect\", 0) or 0 for s in stats] + [s.get(\"completion_tokens_revise\", 0) or 0 for s in stats])\n",
        "\n",
        "    final = {\n",
        "        \"prompt_tokens_final\": prompt_tokens_final,\n",
        "        \"completion_tokens_final\": completion_tokens_final,\n",
        "        \"total_tokens_final\": prompt_tokens_final + completion_tokens_final,\n",
        "        \"price\": (prompt_tokens_final * PRICING[model][\"input\"] + completion_tokens_final * PRICING[model][\"output\"]) / 1000000\n",
        "    }\n",
        "    stats.append(final)\n",
        "    results_df = pd.DataFrame(data)\n",
        "    results_df.to_csv(f\"{dir_path}/amr_parses.tsv\", sep=\"\\t\")\n",
        "    with open(f\"{dir_path}/full_logs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join([str(log) for log in full_logs]))\n",
        "\n",
        "    return stats, gens, drafts\n",
        "\n",
        "\n",
        "def generate_and_eval_langchain(sample, model, comment=\"test\", prepend_to_message=\"\", append_to_message=\"\", **kwargs):\n",
        "    current_time = datetime.now()\n",
        "    filename_time = current_time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    dir_path = f\"/content/gdrive/My Drive/amr_parsing/results/{model}-{filename_time}_{comment}\"\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "    stats, gens, drafts = get_amrs_from_langchain(sample, model, dir_path, prepend_to_message=prepend_to_message, append_to_message=append_to_message, **kwargs)\n",
        "    with open(f\"{dir_path}/stats.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join([str(d) for d in stats]))\n",
        "    golds = get_golds(sample)\n",
        "    valid_pct, smatchpp_score = validate_and_score(golds, gens, report_path=f\"{dir_path}/evals.txt\")\n",
        "    valid_pct_d, smatchpp_score_d = validate_and_score(golds, drafts, report_path=f\"{dir_path}/draft_evals.txt\")\n",
        "    return valid_pct, smatchpp_score, valid_pct_d, smatchpp_score_d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f00dd5",
      "metadata": {},
      "source": [
        "### gpt-4o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LDv8rsP8e9eW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f7783ff3fa144e188ae134a84fa9857e",
            "6232baa9e2204ada9e2cae83270fb2cb",
            "76193ae3d35d45a49ff00dfb8456974b",
            "23b5cc6d4fba42aba721f61fcdf436fe",
            "d7aebd2d59844b57948963e87ffcce47",
            "816a2514f61647cfafa181214d08d87a",
            "381c9a92df314f0fbf82836f8c83d28b",
            "1f96654b6a0d41b7874efdecae82c73d",
            "c4d83d901c1a4147b2447a29bc0c5d4c",
            "51bea7e098294fa18e309005428aba76",
            "50776c37bfe94f0d8f20b6e3a73fc2d3",
            "a2e582364a73424d9a71795ae43d0efb",
            "32a8272831f84098a2c112710cf8faa8",
            "7904e7ec56b84147842b065d7c142afb",
            "a4ba0545cd2c4e9992e08f54e7d7c8f1",
            "6ce80a0839364abdabd9e2c093fd1052",
            "bf4ace4d317c4a598027008f17d5ab09",
            "422ddbe65797426eb7874bddfe919a3e",
            "f8fd416d1d264df591e6549d7eb71e0a",
            "62a50c2bcfe548ddaba9ea11232ae7ea",
            "dab45a2b29a943fa85c45e4040580954",
            "14d0e31846b44da7b62ea8c52e9d4668"
          ]
        },
        "id": "LDv8rsP8e9eW",
        "outputId": "3e90c480-4e87-465f-9d58-a79cade2b74e"
      },
      "outputs": [],
      "source": [
        "generate_and_eval_langchain(sample_test, \"gpt-4o\", comment=\"langchain\", prepend_to_message=\"\", append_to_message=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f09e5d73",
      "metadata": {},
      "source": [
        "### gpt-5-2025-08-07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mCN0SmpoiY0h",
      "metadata": {
        "id": "mCN0SmpoiY0h"
      },
      "outputs": [],
      "source": [
        "generate_and_eval_langchain(sample_test, \"gpt-5-2025-08-07\", comment=\"langchain\", prepend_to_message=\"\", append_to_message=\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
