# AMR Parsing with Large Language Models

Abstract Meaning Representation (AMR) is a semantic representation framework that encodes semantic information as rooted, directed, acyclic graphs. This project explores how modern LLMs perform on AMR parsing tasks using the AMR 3.0 dataset, comparing different prompting strategies and model configurations. 

In this project, I do the following:

- Test different approaches to improve LLM performance on AMR parsing
- Track token usage and API costs across experiments
- Use Smatch++ for AMR parsing evaluation

You may find the paper summarizing the project setup and results [here](https://github.com/smilni/amr-llm-parsing/blob/main/AMR_parsing_with_LLMs.pdf).

## Experiments

The project includes multiple experimental configurations:

- Baseline: 
  - Traditional AMR parser performance
- GPT-4o and GPT-5 experiments:
  - 0-shot prompting
  - 10-shot prompting with random examples
  - 10-shot prompting with relevant examples selected via RAG
  - 247-shot prompting with detailed instruction adapted from official AMR guidelines
  - LangChain self-correcting agent

## Usage

1. Install dependencies: `pip install -r requirements.txt`
2. Set up OpenAI API key in environment variables (`.env` file)
3. Run `prompting_and_analysis.ipynb` for main experiments
4. Use `cost_calculation.ipynb` for token usage analysis

## Project Structure

```
├── data/                           # AMR 3.0 dataset, needs to be obtained manually to run the project
│   ├── {upload you files here}
│   └── AMR_detailed_instruction.txt
├── results/
│   ├── baseline/                  # Baseline AMR parser results
│   └── fin/                       # LLM experiment results
│       ├── gpt-4o-*/              # GPT-4o experiments
│           ├── amr_parses.tsv     # sentences; parses generated by LLM; gold parses
│           ├── evals.txt          # evaluation result (validity & Smatch++ scores)
│           └── stats.txt          # cost statistics: one line is one request; last line is aggregated stats for the run
│       └── gpt-5-*/               # GPT-5 experiments
│           └── ...                # same as for gpt-4o
├── prompting_and_analysis.ipynb   # Main project notebook; contains code used to generate results/
├── cost_calculation.ipynb         # Token usage and cost analysis notebook
└── requirements.txt               # Python dependencies
```

## Results

Results are stored in the `results/` directory with AMR parses, evaluation metrics, AMR parses, and cost summaries for each experimental configuration.
