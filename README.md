# AMR Parsing with Large Language Models

This project evaluates the performance of API-based Large Language Models (LLMs) for Abstract Meaning Representation (AMR) parsing, comparing different prompting strategies and model configurations.

## Overview

Abstract Meaning Representation (AMR) is a semantic representation framework that encodes semantic information as rooted, directed, acyclic graphs. This project explores how modern LLMs perform on AMR parsing tasks using the AMR 3.0 dataset.

## Key Features

- Tests different approaches to improve LLM performance
- Tracks token usage and API costs across experiments
- Uses Smatch++ for AMR parsing evaluation

## Project Structure

```
├── data/                           # AMR 3.0 dataset, needs to be obtained manually to run the project
│   ├── {upload you files here}
│   └── AMR_detailed_instruction.txt
├── results/
│   ├── baseline/                  # Baseline AMR parser results
│   └── fin/                       # LLM experiment results
│       ├── gpt-4o-*/              # GPT-4o experiments
│           ├── amr_parses.tsv     # sentences; parses generated by LLM; gold parses
│           ├── evals.txt          # evaluation result (validity & Smatch++ scores)
│           └── stats.txt          # cost statistics: one line is one request; last line is aggregated stats for the run
│       └── gpt-5-*/               # GPT-5 experiments
│           └── ...                # same as for gpt-4o
├── prompting_and_analysis.ipynb   # Main project notebook; contains code used to generate results/
├── cost_calculation.ipynb         # Token usage and cost analysis notebook
└── requirements.txt               # Python dependencies
```

## Experiments

The project includes multiple experimental configurations:

- Baseline: 
  - Traditional AMR parser performance
- GPT-4o and GPT-5 experiments:
  - 0-shot prompting
  - 10-shot prompting with random examples
  - 10-shot prompting with relevant examples selected via RAG
  - 247-shot prompting with detailed instruction adapted from official AMR guidelines
  - LangChain self-correcting agent

## Usage

1. Install dependencies: `pip install -r requirements.txt`
2. Set up OpenAI API key in environment variables (`.env` file)
3. Run `prompting_and_analysis.ipynb` for main experiments
4. Use `cost_calculation.ipynb` for token usage analysis

## Installation

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd project
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   Create a `.env` file in the project root with your OpenAI API key:
   ```
   OPENAI_API_KEY=your_api_key_here
   ```

## Usage

1. Run `prompting_and_analysis.ipynb` for main experiments
2. Use `cost_calculation.ipynb` for token usage analysis

## Results

Results are stored in the `results/` directory with AMR parses, evaluation metrics, and cost summaries for each experimental configuration.

## Contributing

This is a research project for academic purposes. If you have suggestions or find issues, please feel free to open an issue or submit a pull request.

## License

This project is for academic research purposes. Please cite appropriately if you use this work in your research.

## Citation

If you use this project in your research, please cite:

```bibtex
@misc{amr_llm_parsing,
  title={AMR Parsing with Large Language Models},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/amr-llm-parsing}
}
```

## Acknowledgments

- AMR 3.0 dataset
- OpenAI for providing API access
- Smatch++ evaluation framework
