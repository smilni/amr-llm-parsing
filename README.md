# AMR Parsing with Large Language Models

Abstract Meaning Representation (AMR) is a semantic representation framework that encodes semantic information as rooted, directed, acyclic graphs. This project explores how modern LLMs perform on AMR parsing tasks using the AMR 3.0 dataset, comparing different prompting strategies and model configurations. I do the following:

- Tests different approaches to improve LLM performance on AMR parsing
- Tracks token usage and API costs across experiments
- Uses Smatch++ for AMR parsing evaluation

## Project Structure

```
├── data/                           # AMR 3.0 dataset, needs to be obtained manually to run the project
│   ├── {upload you files here}
│   └── AMR_detailed_instruction.txt
├── results/
│   ├── baseline/                  # Baseline AMR parser results
│   └── fin/                       # LLM experiment results
│       ├── gpt-4o-*/              # GPT-4o experiments
│           ├── amr_parses.tsv     # sentences; parses generated by LLM; gold parses
│           ├── evals.txt          # evaluation result (validity & Smatch++ scores)
│           └── stats.txt          # cost statistics: one line is one request; last line is aggregated stats for the run
│       └── gpt-5-*/               # GPT-5 experiments
│           └── ...                # same as for gpt-4o
├── prompting_and_analysis.ipynb   # Main project notebook; contains code used to generate results/
├── cost_calculation.ipynb         # Token usage and cost analysis notebook
└── requirements.txt               # Python dependencies
```

## Experiments

The project includes multiple experimental configurations:

- Baseline: 
  - Traditional AMR parser performance
- GPT-4o and GPT-5 experiments:
  - 0-shot prompting
  - 10-shot prompting with random examples
  - 10-shot prompting with relevant examples selected via RAG
  - 247-shot prompting with detailed instruction adapted from official AMR guidelines
  - LangChain self-correcting agent

## Usage

1. Install dependencies: `pip install -r requirements.txt`
2. Set up OpenAI API key in environment variables (`.env` file)
3. Run `prompting_and_analysis.ipynb` for main experiments
4. Use `cost_calculation.ipynb` for token usage analysis

## Results

Results are stored in the `results/` directory with AMR parses, evaluation metrics, AMR parses, and cost summaries for each experimental configuration.
